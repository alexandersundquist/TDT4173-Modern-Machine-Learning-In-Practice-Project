{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/datasets/ais_train.csv', sep='|')\n",
    "train['time'] = pd.to_datetime(train['time'])\n",
    "train.info()\n",
    "missing_values = train.isnull().sum()\n",
    "print(\"Number of missing values in each column:\\n\", missing_values)\n",
    "\n",
    "train.head()\n",
    "\n",
    "test = pd.read_csv('data/datasets/ais_test.csv', sep=',')\n",
    "test['time'] = pd.to_datetime(test['time'])\n",
    "test.head()\n",
    "\n",
    "vessels = pd.read_csv('data/datasets/vessels.csv', sep='|')\n",
    "vessels.head()\n",
    "\n",
    "ports = pd.read_csv('data/datasets/ports.csv', sep='|')\n",
    "ports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna(subset=['portId'])\n",
    "print(train.shape)\n",
    "# Verify the result by checking the count of missing values\n",
    "print(\"Number of missing values in each column after dropping rows with NaN in 'portId':\")\n",
    "print(train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules = pd.read_csv('data/datasets/schedules_to_may_2024.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_id_mapping = {port_id: idx for idx, port_id in enumerate(train['portId'].unique())}\n",
    "train['portId'] = train['portId'].map(port_id_mapping)\n",
    "ports['portId'] = ports['portId'].map(port_id_mapping)\n",
    "schedules['portId'] = schedules['portId'].map(port_id_mapping)\n",
    "\n",
    "vessel_id_mapping = {vessel_id: idx for idx, vessel_id in enumerate(train['vesselId'].unique())}\n",
    "train['vesselId'] = train['vesselId'].map(vessel_id_mapping)\n",
    "vessels['vesselId'] = vessels['vesselId'].map(vessel_id_mapping)\n",
    "test['vesselId'] = test['vesselId'].map(vessel_id_mapping)\n",
    "schedules['vesselId'] = schedules['vesselId'].map(vessel_id_mapping)\n",
    "\n",
    "\n",
    "shipping_line_id_mapping = {shipping_line_id: idx for idx, shipping_line_id in enumerate(vessels['shippingLineId'].unique())}\n",
    "vessels['shippingLineId'] = vessels['shippingLineId'].map(shipping_line_id_mapping)\n",
    "schedules['shippingLineId'] = schedules['shippingLineId'].map(shipping_line_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytz\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m schedules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msailingDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mto_datetime(schedules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msailingDate\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m schedules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrivalDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(schedules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marrivalDate\u001b[39m\u001b[38;5;124m'\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Convert sailingDate and arrivalDate to a different timezone, such as CET (Central European Time)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Convert datetime columns to timezone-naive format by removing timezone info\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import pytz\n",
    "schedules['sailingDate'] = pd.to_datetime(schedules['sailingDate'], errors='coerce')\n",
    "schedules['arrivalDate'] = pd.to_datetime(schedules['arrivalDate'], errors='coerce')\n",
    "\n",
    "# Convert sailingDate and arrivalDate to a different timezone, such as CET (Central European Time)\n",
    "# Convert datetime columns to timezone-naive format by removing timezone info\n",
    "schedules['sailingDate'] = schedules['sailingDate'].dt.tz_convert(None)\n",
    "schedules['arrivalDate'] = schedules['arrivalDate'].dt.tz_convert(None)\n",
    "\n",
    "\n",
    "schedules.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train = train.merge(ports[['portId', 'latitude', 'longitude']], how='left', left_on='portId', right_on='portId', suffixes=('', '_port'))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(vessels[['vesselId', 'length', 'shippingLineId', 'breadth', 'GT']], on='vesselId', how='left')\n",
    "train['vessel_deep_sea'] = np.where(train['length'] > 200, 1, 0)\n",
    "\n",
    "num_maxSpeed_nan = train['breadth'].isna().sum()\n",
    "print(f\"Number of NaN values in maxSpeed: {num_maxSpeed_nan}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean breadth for shipping line 5\n",
    "mean_breadth_shipping_line_5 = train[train['shippingLineId'] == 5]['breadth'].mean()\n",
    "\n",
    "# Impute missing breadth with the calculated mean\n",
    "train.loc[train['shippingLineId'] == 5, 'breadth'] = train.loc[train['shippingLineId'] == 5, 'breadth'].fillna(mean_breadth_shipping_line_5)\n",
    "\n",
    "# Verify if the missing values were filled\n",
    "missing_count_after_imputation = train['breadth'].isnull().sum()\n",
    "print(f\"Missing Breadth values after imputation: {missing_count_after_imputation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()\n",
    "missing_values = train.isnull().sum()\n",
    "print(\"Number of missing values in each column:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_days_before_prediction = train.copy()\n",
    "# Ensure 'time' column is in datetime format\n",
    "df_days_before_prediction['time'] = pd.to_datetime(df_days_before_prediction['time'])\n",
    "\n",
    "\n",
    "# Specify the year to append\n",
    "current_year = 2024  # Change to the desired year\n",
    "\n",
    "# Modify etaRaw to include the year\n",
    "df_days_before_prediction['etaRaw'] = df_days_before_prediction['etaRaw'].apply(lambda x: f\"{current_year}-{x}\")\n",
    "\n",
    "\n",
    "# Convert 'etaRaw' column to datetime\n",
    "df_days_before_prediction['etaRaw'] = pd.to_datetime(df_days_before_prediction['etaRaw'], format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "\n",
    "# Define the start of the prediction period and calculate minimum required date\n",
    "prediction_start_date = df_days_before_prediction['time'].max()\n",
    "days_before_start = 0  # Set to the number of days you want before the start date\n",
    "min_required_date = prediction_start_date \n",
    "\n",
    "# Get the last recorded date for each vessel along with `etaRaw`\n",
    "vessel_last_dates = df_days_before_prediction.groupby('vesselId').agg({'time': 'max', 'etaRaw': 'last'}).reset_index()\n",
    "vessel_last_dates.rename(columns={'time': 'last_train_time'}, inplace=True)\n",
    "\n",
    "# Check the minimum required date\n",
    "print(\"\\nMinimum required date:\")\n",
    "print(min_required_date)\n",
    "\n",
    "# Add a new column to indicate if etaRaw is before the minimum required date\n",
    "vessel_last_dates['etaRaw_before_required'] = vessel_last_dates['etaRaw'] < min_required_date\n",
    "\n",
    "# Calculate days missing from the required date\n",
    "vessel_last_dates['days_from_required'] = (min_required_date - vessel_last_dates['last_train_time']).dt.days\n",
    "\n",
    "# Add a new column to indicate if the data is before the required date\n",
    "vessel_last_dates['before_required_date'] = vessel_last_dates['days_from_required'] > 1\n",
    "\n",
    "# Separate vessels with incomplete data\n",
    "vessels_with_incomplete_data = vessel_last_dates[vessel_last_dates['before_required_date']]\n",
    "\n",
    "# Plot vessels with incomplete data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(vessels_with_incomplete_data['vesselId'], vessels_with_incomplete_data['days_from_required'], color='orange')\n",
    "plt.xlabel('Days from Required Date')\n",
    "plt.ylabel('Vessel ID')\n",
    "plt.title('Days Missing from Required Prediction Period by Vessel')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the vessel with the most missing days on top\n",
    "plt.grid(axis='x')  # Add gridlines for better readability\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the minimum required date for predictions in the test set\n",
    "min_required_date = test['time'].min() - pd.Timedelta(days=5)  # 5 days before the earliest test date\n",
    "print(f\"Minimum required date for predictions: {min_required_date}\")\n",
    "\n",
    "# Filter the schedules DataFrame based on the required date\n",
    "schedules_filtered = schedules[~((schedules['sailingDate'] < min_required_date) & \n",
    "                                  (schedules['arrivalDate'] < min_required_date))]\n",
    "\n",
    "# Get the last recorded date for each vessel in the training set\n",
    "last_train_dates = train.groupby('vesselId')['time'].max().reset_index()\n",
    "last_train_dates.rename(columns={'time': 'last_train_time'}, inplace=True)\n",
    "\n",
    "# Print the filtered schedules for verification\n",
    "# print(\"Filtered schedules:\")\n",
    "# print(schedules_filtered.head())\n",
    "\n",
    "# Filter vessels that are in the test set\n",
    "vessels_in_test = test['vesselId'].unique()\n",
    "missing_days_df = last_train_dates[last_train_dates['vesselId'].isin(vessels_in_test)].copy()\n",
    "\n",
    "# Calculate the days missing for each vessel from the required date\n",
    "missing_days_df['days_missing'] = (min_required_date - missing_days_df['last_train_time']).dt.days\n",
    "\n",
    "# Filter vessels missing a significant number of days (e.g., more than 2 days)\n",
    "significant_gap_vessels = missing_days_df[missing_days_df['days_missing'] > 1]\n",
    "\n",
    "# Print the vessels with significant gaps\n",
    "print(\"Vessels in the test set missing significant data before prediction date:\")\n",
    "print(significant_gap_vessels[['vesselId', 'last_train_time', 'days_missing']])\n",
    "print(f\"Number of vessels with significant missing days: {len(significant_gap_vessels)}\")\n",
    "\n",
    "# Optional: If you want to check which vessels are in filtered schedules\n",
    "vessels_in_filtered_schedules = schedules_filtered['vesselId'].unique()\n",
    "print(f\"Number of unique vessels in filtered schedules: {len(vessels_in_filtered_schedules)}\")\n",
    "\n",
    "# Step to find which significant gap vessels are in filtered schedules\n",
    "vessels_in_gap_and_schedule = set(significant_gap_vessels['vesselId']).intersection(vessels_in_filtered_schedules)\n",
    "\n",
    "# Create a DataFrame for common vessels with significant gaps in schedules\n",
    "vessels_in_gap_and_schedule_df = significant_gap_vessels[significant_gap_vessels['vesselId'].isin(vessels_in_gap_and_schedule)]\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"Vessels with significant gaps that are also in the filtered schedules:\")\n",
    "print(vessels_in_gap_and_schedule_df[['vesselId', 'last_train_time', 'days_missing']])\n",
    "print(f\"Number of vessels with significant missing days that are in schedules: {len(vessels_in_gap_and_schedule_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step to find which significant gap vessels are NOT in filtered schedules\n",
    "vessels_in_gap_not_in_schedule = set(significant_gap_vessels['vesselId']).difference(vessels_in_filtered_schedules)\n",
    "\n",
    "# Create a DataFrame for vessels with significant gaps that are not in the schedules\n",
    "vessels_in_gap_not_in_schedule_df = significant_gap_vessels[significant_gap_vessels['vesselId'].isin(vessels_in_gap_not_in_schedule)]\n",
    "\n",
    "# Print results\n",
    "print(\"Vessels with significant gaps that are NOT in the filtered schedules:\")\n",
    "print(vessels_in_gap_not_in_schedule_df[['vesselId', 'last_train_time', 'days_missing']])\n",
    "print(f\"Number of vessels with significant missing days that are NOT in schedules: {len(vessels_in_gap_not_in_schedule_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_five_day_windows(df):\n",
    "    # Ensure that 'time' column is in datetime format\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    \n",
    "    # Sort data to ensure time sequence within each vessel\n",
    "    df = df.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "    \n",
    "    window_size_days = 5\n",
    "    windows = []\n",
    "\n",
    "    for vessel_id, group in df.groupby('vesselId'):\n",
    "        group = group.sort_values(by='time')\n",
    "        unique_dates = group['time'].dt.date.unique()\n",
    "\n",
    "        for start_idx in range(len(unique_dates) - window_size_days + 1):\n",
    "            start_date = unique_dates[start_idx]\n",
    "            end_date = unique_dates[start_idx + window_size_days - 1]\n",
    "            window = group[(group['time'].dt.date >= start_date) & (group['time'].dt.date <= end_date)]\n",
    "\n",
    "            # Skip empty windows\n",
    "            if len(window) == 0:\n",
    "                continue\n",
    "\n",
    "            # Reference row: the first row in the window\n",
    "            reference_row = window.iloc[0]\n",
    "\n",
    "            # Append each row in this window with reference features from the first row\n",
    "            for _, row in window.iterrows():\n",
    "                windows.append({\n",
    "                    'vesselId': row['vesselId'],\n",
    "                    'time': row['time'],\n",
    "                    'latitude': row['latitude'],\n",
    "                    'longitude': row['longitude'],\n",
    "                    'cog_sin': row['cog_sin'],\n",
    "                    'cog_cos': row['cog_cos'],\n",
    "                    'cog' : row['cog'],\n",
    "                    'sog': row['sog'],\n",
    "                    'rot': row['rot'],\n",
    "                    'under_way': row['under_way'],\n",
    "                    'length' : row['length'],\n",
    "                    'breadth': row['breadth'],\n",
    "                    # 'DWT': row['DWT'],\n",
    "                    'GT': row['GT'],\n",
    "                    # 'vessel_deep_sea': row['vessel_deep_sea'],\n",
    "                    # Features based on the first row in the window\n",
    "                    'latitude_first': reference_row['latitude'],\n",
    "                    'longitude_first': reference_row['longitude'],\n",
    "                    'cog_sin_first': reference_row['cog_sin'],\n",
    "                    'cog_cos_first': reference_row['cog_cos'],\n",
    "                    'sog_first': reference_row['sog'],\n",
    "                    'rot_first': reference_row['rot'],\n",
    "                    'under_way_first': reference_row['under_way'],\n",
    "                    'time_since_start': (row['time'] - reference_row['time']).total_seconds()\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a)) \n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return r * c  # Distance in kilometers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPoint\n",
    "\n",
    "def calculate_convex_hull_area(lats, lons):\n",
    "    \"\"\"Calculate the area of the convex hull formed by the given latitudes and longitudes.\"\"\"\n",
    "    # Create a list of (longitude, latitude) points\n",
    "    points = [(lon, lat) for lon, lat in zip(lons, lats)]\n",
    "    \n",
    "    # If there are fewer than 3 unique points, the area is zero\n",
    "    if len(points) < 3 or len(set(points)) < 3:\n",
    "        return 0.0\n",
    "\n",
    "    # Use MultiPoint to calculate the convex hull area\n",
    "    multipoint = MultiPoint(points)\n",
    "    hull = multipoint.convex_hull\n",
    "    \n",
    "    # Return the area of the convex hull\n",
    "    return hull.area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def future_position(lat, lon, cog, sog, time_diff):\n",
    "    \"\"\"\n",
    "    Calculate the future position given the current latitude, longitude, COG, SOG, and time difference in seconds.\n",
    "    \"\"\"\n",
    "    # Convert COG from degrees to radians\n",
    "    cog_rad = np.radians(cog)\n",
    "\n",
    "    # Calculate distance traveled in meters\n",
    "    distance = sog * time_diff  # distance = speed * time\n",
    "\n",
    "    # Calculate future latitude and longitude\n",
    "    future_lat = lat + (distance * np.cos(cog_rad)) / 111320  # 111320 meters per degree latitude\n",
    "    future_lon = lon + (distance * np.sin(cog_rad)) / (111320 * np.cos(np.radians(lat)))  # adjust for longitude\n",
    "\n",
    "    return future_lat, future_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(train):\n",
    "    train = train.copy()\n",
    "    train = train.sort_values(by=['vesselId', 'time']).reset_index(drop=True)\n",
    "\n",
    "    # # Handle NaNs\n",
    "    # train[['latitude', 'longitude', 'cog', 'time']] = (\n",
    "    #     train.groupby('vesselId')[['latitude', 'longitude', 'cog', 'time']].apply(lambda x: x.ffill().bfill())\n",
    "    # ).reset_index(drop=True)\n",
    "\n",
    "    features = pd.DataFrame()\n",
    "    features['vesselId'] = train['vesselId']\n",
    "    features['time'] = train['time'] \n",
    "    features['latitude'] = train['latitude']\n",
    "    features['longitude'] = train['longitude']\n",
    "    features['cog'] = train['cog']\n",
    "    features['sog'] = train['sog']\n",
    "    features['rot'] = train['rot']\n",
    "    features['under_way'] = train['navstat'].isin([0, 8]).astype(int)\n",
    "    features['cog_rad'] = np.radians(features['cog'])\n",
    "    features['cog_sin'] = np.sin(features['cog_rad'])\n",
    "    features['cog_cos'] = np.cos(features['cog_rad'])\n",
    "    features['length'] = train['length']\n",
    "    features['breadth'] = train['breadth']\n",
    "    features['GT'] = train['GT']\n",
    "    \n",
    "    features = create_five_day_windows(features)\n",
    "\n",
    "    features['latitude_port'] = train['latitude_port']\n",
    "    features['longitude_port'] = train['longitude_port']\n",
    "\n",
    "    features['distance_to_port'] = haversine(features['latitude'], features['longitude'], \n",
    "                                              features['latitude_port'], features['longitude_port'])\n",
    "    \n",
    "#     features['time_diff'] = features.groupby('vesselId')['time'].diff().dt.total_seconds().fillna(0)\n",
    "\n",
    "#     # # Calculate future positions\n",
    "#     # future_positions = features.apply(lambda row: future_position(\n",
    "#     #     row['latitude'], row['longitude'], row['cog'], row['sog'], row['time_diff']), axis=1)\n",
    "\n",
    "#     # features['future_latitude'], features['future_longitude'] = zip(*future_positions)\n",
    "    \n",
    "#     features['area_covered'] = features.apply(lambda row: calculate_convex_hull_area(\n",
    "#         features[(features['vesselId'] == row['vesselId']) & (features['time'] <= row['time'])]['latitude'].tolist(),\n",
    "#         features[(features['vesselId'] == row['vesselId']) & (features['time'] <= row['time'])]['longitude'].tolist()), axis=1)\n",
    "\n",
    "#     # Calculate cumulative area covered for each vessel\n",
    "#     features['cumulative_area_covered'] = features.groupby('vesselId')['area_covered'].cumsum()\n",
    " \n",
    "    # Additional time-based features\n",
    "    features['month'] = features['time'].dt.month\n",
    "    features['day'] = features['time'].dt.day\n",
    "    features['hour'] = features['time'].dt.hour\n",
    "    features['minute'] = features['time'].dt.minute\n",
    "    # features['day_of_week'] = features['time'].dt.dayofweek\n",
    "\n",
    "    return features\n",
    "train_features = feature_engineering(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_values = train_features.isnull().sum()\n",
    "print(nan_values)\n",
    "\n",
    "\n",
    "# Drop the columns 'time' and 'rot'\n",
    "train_features = train_features.drop(columns=['time', 'rot_first'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.dropna()\n",
    "\n",
    "train_features.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_features(features):\n",
    "    # Assuming that the last row for every vesselId is the most recent\n",
    "    last_features = features.groupby('vesselId').last().reset_index()\n",
    "    return last_features\n",
    "\n",
    "last_features = find_last_features(train_features)\n",
    "# last_features.drop(columns=['DWT']) \n",
    "last_features.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define features and target\n",
    "y = train_features[['latitude', 'longitude']]  \n",
    "X = train_features.drop(columns=['latitude', 'longitude']) \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = RandomForestRegressor(max_depth=50, random_state=42)\n",
    "\n",
    "# # Define the parameter grid for Random Forest\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 300],            # Number of trees in the forest\n",
    "#     'max_depth': [5, 10, 25, None],             # Maximum depth of the tree\n",
    "#     'min_samples_split': [2, 5],           # Minimum number of samples required to split an internal node\n",
    "#     'min_samples_leaf': [1, 2],            # Minimum number of samples required to be at a leaf node\n",
    "#     'max_features': ['auto', 'sqrt'],      # The number of features to consider when looking for the best split\n",
    "#     'bootstrap': [True, False]             # Whether bootstrap samples are used when building trees\n",
    "# }\n",
    "\n",
    "# # Perform Randomized Search\n",
    "# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
    "#                                    n_iter=20, scoring='neg_mean_squared_error', cv=5, verbose=1, random_state=42)\n",
    "# random_search.fit(X, y)\n",
    "\n",
    "# best_model = random_search.best_estimator_\n",
    "\n",
    "# # Output the best parameters and score\n",
    "# print(f\"Best parameters: {random_search.best_params_}\")\n",
    "# print(f\"Best score (negative mean squared error): {-random_search.best_score_}\")\n",
    "\n",
    "best_model = xgb.XGBRegressor()\n",
    "\n",
    "# Fit the model\n",
    "best_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Get feature importances\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Step 2: Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "print(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Get feature importances\n",
    "feature_importances = best_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Step 2: Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "print(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm unique vesselIds in vessels_in_gap_in_schedule\n",
    "print(\"Unique vesselIds in vessels_in_gap_in_schedule:\")\n",
    "print(vessels_in_gap_and_schedule_df['vesselId'].unique())\n",
    "print(f\"Count of unique vesselIds: {len(vessels_in_gap_and_schedule_df['vesselId'].unique())}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_vessels = set(last_features['vesselId']).intersection(set(vessels_in_gap_and_schedule_df['vesselId']))\n",
    "print(f\"Number of matching vesselIds: {len(matching_vessels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_features_gap = last_features[last_features['vesselId'].isin(vessels_in_gap_not_in_schedule_df['vesselId'])].copy()\n",
    "last_features_gap.head()\n",
    "\n",
    "last_features_gap['under_way'] = 0  # Stationary mode\n",
    "last_features_gap['sog'] = 0        # Speed over ground to 0\n",
    "last_features_gap['rot'] = 0        # Rate of turn to 0\n",
    "last_features_gap['cog'] = 0        # Optional: course over ground to 0\n",
    "last_features_gap['cog_sin'] = 0    # Sine of course to 0\n",
    "last_features_gap['cog_cos'] = 1    # Cosine of course to 1\n",
    "last_features_gap['distance_to_port'] = 0\n",
    "last_features_gap['time_since_start'] = 0\n",
    "\n",
    "\n",
    "# Align with the maximum time and date fields in last_features\n",
    "last_features_gap['day'] = 6 #sunday before 8th of May \n",
    "last_features_gap['month'] = last_features['month'].max()\n",
    "# last_features_gap['minute'] = last_features['minute'].max()\n",
    "last_features_gap = last_features_gap.drop(columns=['year'], errors='ignore')\n",
    "\n",
    "# Explicitly cast each column in last_features_gap to match last_features data types\n",
    "for column in last_features_gap.columns:\n",
    "    if column in last_features.columns:\n",
    "        last_features_gap[column] = last_features_gap[column].astype(last_features[column].dtype)\n",
    "\n",
    "\n",
    "last_features_gap.columns\n",
    "last_features_gap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in last_features.columns:\n",
    "    if column in last_features_gap.columns:\n",
    "        last_features_gap[column] = last_features_gap[column].astype(last_features[column].dtype)\n",
    "\n",
    "# Set `vesselId` as the index for both DataFrames to ensure alignment on `vesselId`\n",
    "last_features.set_index('vesselId', inplace=True)\n",
    "last_features_gap.set_index('vesselId', inplace=True)\n",
    "\n",
    "# Update existing rows in `last_features` with values from `last_features_gap`\n",
    "last_features.update(last_features_gap)\n",
    "\n",
    "# Use `combine_first` to add any missing `vesselId`s from `last_features_gap` to `last_features`\n",
    "last_features = last_features.combine_first(last_features_gap)\n",
    "\n",
    "# Reset the index to restore `vesselId` as a column\n",
    "last_features.reset_index(inplace=True)\n",
    "last_features_gap.reset_index(inplace=True)\n",
    "\n",
    "# Check a sample to verify the updates\n",
    "print(last_features[last_features['vesselId'].isin(vessels_in_gap_not_in_schedule_df['vesselId'])].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rows with any NaN values in last_features\n",
    "nan_rows = last_features[last_features.isna().any(axis=1)]\n",
    "print(nan_rows)\n",
    "\n",
    "# Alternatively, get a summary of NaNs in each column\n",
    "nan_summary = last_features.isna().sum()\n",
    "print(\"Summary of NaN values per column:\")\n",
    "print(nan_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_features.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vessel_ids = test[~test['vesselId'].isin(last_features['vesselId'])]\n",
    "print(\"Missing vessel IDs:\", missing_vessel_ids['vesselId'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare test data for predictions\n",
    "expected_features = X.columns \n",
    "\n",
    "def prepare_test_for_predictions(test, last_features):\n",
    "    test = test.copy()\n",
    "    prepared_test = pd.DataFrame()\n",
    "\n",
    "    # Create a time column in last features\n",
    "    last_features['year'] = 2024\n",
    "    last_features['time'] = pd.to_datetime(\n",
    "        last_features[['year', 'month', 'day', 'hour', 'minute']]\n",
    "    )\n",
    "\n",
    "    # Add the columns vesselId and time\n",
    "    prepared_test['vesselId'] = test['vesselId']\n",
    "    prepared_test['time'] = test['time']\n",
    "    print(prepared_test.head())\n",
    "    # For each vessel, add the last seen features to the prepared test\n",
    "    prepared_test = prepared_test.merge(last_features, on='vesselId', how='left', suffixes=('', '_last'))\n",
    "\n",
    "    print(prepared_test.head())\n",
    "\n",
    "    # Move the last_features to the reference row in the windows\n",
    "    prepared_test['latitude_first'] = prepared_test['latitude']\n",
    "    prepared_test['longitude_first'] = prepared_test['longitude']\n",
    "    prepared_test['cog_sin_first'] = prepared_test['cog_sin']\n",
    "    prepared_test['cog_cos_first'] = prepared_test['cog_cos']  # Fixed typo here\n",
    "    prepared_test['sog_first'] = prepared_test['sog']\n",
    "    prepared_test['under_way_first'] = prepared_test['under_way']\n",
    "    prepared_test['time_since_start'] = (prepared_test['time'] - prepared_test['time_last']).dt.total_seconds()\n",
    "\n",
    "    # Split the time column into month, day, hour, minute\n",
    "    prepared_test['month'] = test['time'].dt.month\n",
    "    prepared_test['day'] = test['time'].dt.day\n",
    "    prepared_test['hour'] = test['time'].dt.hour\n",
    "    prepared_test['minute'] = test['time'].dt.minute\n",
    "\n",
    "    prepared_test['length'] = prepared_test['length']  # Ensure this column exists in 'prepared_test'\n",
    "    prepared_test['breadth'] = prepared_test['breadth']  # Ensure this column exists in 'prepared_test'\n",
    "    # prepared_test['DWT'] = prepared_test['DWT']  # Ensure this column exists in 'prepared_test'\n",
    "    prepared_test['GT'] = prepared_test['GT']  # Ensure this column exists in 'prepared_test'\n",
    "    # prepared_test['vessel_deep_sea'] = prepared_test['vessel_deep_sea']  # Ensure this column exists in 'prepared_test'\n",
    "    prepared_test['rot'] = prepared_test['rot']  # Ensure this column exists in 'prepared_test'\n",
    "    # prepared_test['day_of_week'] = prepared_test['time'].dt.dayofweek  # Added missing day_of_week calculation\n",
    "\n",
    "    # Drop the columns that are no longer needed\n",
    "    prepared_test.drop(['time', 'time_last', 'year'], axis=1, inplace=True)\n",
    "\n",
    "    # Reorder the columns\n",
    "    for column in expected_features:\n",
    "        if column not in prepared_test.columns:\n",
    "            prepared_test[column] = 0  # Assign a default value (0) for missing columns\n",
    "\n",
    "    # Reorder columns to match expected_features\n",
    "    prepared_test = prepared_test[expected_features]\n",
    "\n",
    "    return prepared_test\n",
    "\n",
    "# Prepare the test DataFrame\n",
    "test_df = prepare_test_for_predictions(test, last_features)\n",
    "test_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(test_df)\n",
    "print(predictions)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions, columns=['latitude_predicted', 'longitude_predicted'])\n",
    "predictions_df['ID'] = range(len(predictions_df))\n",
    "predictions_df = predictions_df[['ID', 'longitude_predicted', 'latitude_predicted']]\n",
    "\n",
    "# Save to CSV\n",
    "predictions_df.to_csv('data/submissions/predictions_7_5d_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'test' and 'predictions_df' are already defined and contain the necessary data\n",
    "merged_df = pd.merge(test, predictions_df, on='ID', how='left')\n",
    "\n",
    "# Print the columns to verify\n",
    "print(\"Columns in merged_df before dropping:\", merged_df.columns.tolist())\n",
    "\n",
    "# Drop the specified columns, checking if they exist first\n",
    "columns_to_drop = ['ID', 'scaling_factor']\n",
    "for col in columns_to_drop:\n",
    "    if col in merged_df.columns:\n",
    "        merged_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Alternatively, you can drop them directly with error handling\n",
    "# merged_df.drop(columns=[col for col in columns_to_drop if col in merged_df.columns], inplace=True)\n",
    "\n",
    "# Print the columns after the drop\n",
    "print(\"Columns in merged_df after dropping:\", merged_df.columns.tolist())\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_csv('data/submissions/predictions_7_5d_rf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('data/submissions/plotting_boats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
